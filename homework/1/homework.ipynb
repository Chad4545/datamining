{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  201111774 응용통계학과 박성진\n",
    "\n",
    "## (Homework)                                  \n",
    "\n",
    "### Search the origin of the word \"regression\" by googling and then find the actual meaning of the word at that time\n",
    "\n",
    "- The term __regression__ was coined by Francis Galton in the nineteenth century to describe a biological phenomenon.\n",
    "\n",
    " 회귀라는 용어는 19 세기에 Francis Galton에 의해 생물학적 현상을 설명하기 위해 만들어졌습니다. \n",
    " \n",
    " \n",
    "- The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).\n",
    "\n",
    " 이 현상은 키가 큰 조상의 자손의 높이가 평균(회귀라는 의미의 현상)으로 퇴보하는 경향이 있다는 것을 나타냅니다.\n",
    " \n",
    " \n",
    "\n",
    "- For Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context.\n",
    "\n",
    " Galton의 경우 회귀는 생물학적 의미만 가지고 있었지만 그의 연구는 나중에 Udny Yule과 Karl Pearson에 의해보다 일반적인 통계적 맥락으로 확장되었습니다.\n",
    " \n",
    "---\n",
    "\n",
    "## (Homework) \n",
    "\n",
    "### 1. 3 Interpretation of the model\n",
    "\n",
    "$$ y=f(x)+\\epsilon = B_0 + B_1x +\\epsilon $$\n",
    "\n",
    " - model $f(x)$: conditional expectation of $y \\,given\\,x$\n",
    "\n",
    " - intercept $B_0$ : conditional mean of y given $x=0$\n",
    " $$B_0 = E(y|x = 0) $$\n",
    "\n",
    " - slope $B_1$: difference between two conditional expectations\n",
    " $$B_1 = E(y|x = k+1) - E(y|x=k) ,\\quad \\forall k \\in\\mathbb{R}$$\n",
    "    In this sense, we often say $B_1$ is the effect of x on y\n",
    "    \n",
    "---\n",
    " \n",
    "### 1. 3 .1 Centering/ scaling / standardization   \n",
    "\n",
    "---\n",
    "\n",
    "###  __Centering__ $x$ often helps the interpretation, here centering implies subsituting\n",
    "\n",
    "$$z = x - \\mu_x$$\n",
    "\n",
    "- for $x$, where $\\mu_x = E(x)$ so that the model becomes\n",
    "\n",
    "$$ y=f(x)+\\epsilon = B_0 + B_1x +\\epsilon \\iff y = g(z) = \\alpha_0 + \\alpha_1z + \\epsilon$$\n",
    "\n",
    "- model $g(z)$: conditional expectation of $y \\,given\\,z$\n",
    " \n",
    "$$ g(z) = \\alpha_0 + \\alpha_1z $$\n",
    " \n",
    "---\n",
    "\n",
    "- intercept $\\alpha_0$ : conditional mean of y given $z=0 \\iff x = \\mu_x$\n",
    "\\begin{eqnarray}\n",
    "    \\alpha_0 &=& E(y|z=0) \\\\\n",
    "    &=& E(y|x=\\mu_x) \\\\\n",
    "    &=&B_0 + B_1\\mu_x\n",
    "\\end{eqnarray}        \n",
    "\n",
    "- slope $\\alpha_1$: difference between two conditional expectations\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\alpha_1 &=& E(y|z=k+1) - E(y|z=k)\\\\\n",
    "       &=&E(y|x=\\mu_x+k+1) - E(y|x=\\mu_x+k)\\\\\n",
    "       &=& B_1\n",
    "\\end{eqnarray}\n",
    "\n",
    "--- \n",
    "\n",
    "###  __Scaling__ $x$ implies subsituting\n",
    "\n",
    "$$z = x /\\sigma_x$$\n",
    "\n",
    "- for $\\sigma_x^2 = Var(x)$\n",
    "\n",
    "\n",
    "$$ y=f(x)+\\epsilon = B_0 + B_1x +\\epsilon \\iff y = g(z) = \\alpha_0 + \\alpha_1z + \\epsilon$$\n",
    "\n",
    " - model $g(z)$: conditional expectation of $y \\,given\\,z$\n",
    " \n",
    "  $$ g(z) = \\alpha_0 + \\alpha_1z $$\n",
    "\n",
    " - intercept $\\alpha_0$ : conditional mean of y given $z=0 \\iff x = 0$\n",
    " \n",
    "\\begin{eqnarray}\n",
    "    \\alpha_0 &=& E(y|z=0) \\\\\n",
    "    &=& E(y|x=0)=B_0 \n",
    "\\end{eqnarray}\n",
    "\n",
    " - slope $\\alpha_1$: difference between two conditional expectations\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\alpha_1 &=&E(y|z=k+1) - E(y|z=k)\\\\\n",
    "     &=&E(y|x=\\sigma_x(k+1)) - E(y|x=\\sigma_x(k))\\\\\n",
    "    &=&(B_0 +\\sigma_xB_1k+B_1\\sigma_x) - (B_0 +\\sigma_xB_1k)\\\\\n",
    "    &=&\\sigma_xB_1\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### __Standardization__  $x$ implies subsituting\n",
    " \n",
    "$$z = (x-\\mu_x) /\\sigma_x$$\n",
    "\n",
    "- for $x$, where $\\mu_x = E(x),\\, \\sigma_x^2 = Var(x)$\n",
    "\n",
    "\n",
    "\n",
    "$$ y=f(x)+\\epsilon = B_0 + B_1x +\\epsilon \\iff y = g(z) = \\alpha_0 + \\alpha_1z + \\epsilon$$\n",
    "\n",
    " - model $g(z)$: conditional expectation of $y \\,given\\,z$\n",
    " \n",
    "  $$ g(z) = \\alpha_0 + \\alpha_1z $$\n",
    "\n",
    " - intercept $\\alpha_0$ : conditional mean of y given $z=0 \\iff x = \\mu_x$\n",
    "\\begin{eqnarray} \n",
    "\\alpha_0 &=& E(y|z=0)\\\\ \n",
    "    &=& E(y|x=\\mu_x)=B_0 \n",
    "\\end{eqnarray}\n",
    " - slope $\\alpha_1$: difference between two conditional expectations\n",
    " \n",
    "\\begin{eqnarray}\n",
    "\\alpha_1 &=& E(y|z=k+1) - E(y|z=k)\\\\\n",
    "    &=&E(y|x=\\sigma_x(k+1) + \\mu_x) - E(y|x=\\sigma_x(k)+ \\mu_x)\\\\\n",
    "    &=& (B_0 +\\sigma_xB_1k+B_1\\sigma_x+ \\mu_x) - (B_0 +\\sigma_xB_1k+ \\mu_x)\\\\\n",
    "    &=&\\sigma_xB_1\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201111774 응용통계학과 박성진\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Estimation based on a loss function\n",
    "\n",
    "- ### Steps for estimating two parameters(coefficients) $B_0$ and $B_1$\n",
    "\n",
    "Determine an appropriate loss function  $L(B_0,B_1)$\n",
    "\n",
    "Find the population mimimizer $ \\hat{B}^{pop}_0$ and $ \\hat{B}^{pop}_1$ by minimizing the risk function\n",
    "\n",
    "$$ R(B_0,B_1) = E\\{L(B_0,B_1)\\} $$\n",
    "\n",
    "with respect to $B_0$ and $B_1$\n",
    "\n",
    "here, note that we often cannot determine the minimizer for some possible reason\n",
    "\n",
    "---\n",
    "\n",
    "- ### For example, we can see that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "(\\hat{B}^{pop}_0,\\hat{B}^{pop}_1) &=& \\operatorname{argmin}_{B_0 \\,,B_1} E(y-B_0-B_1x)^2 \\\\\n",
    "    &=&(\\mu_y - \\hat{B}_1 \\mu_x \\,, \\sigma^2_{xy} \\,/ \\,\\sigma^2_x )  \\qquad\\qquad\\cdots{(6)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "where $\\mu_y = E(y) \\, ,\\sigma^2_{xy}=Cov(x,\\,y) $  and $\\sigma^2_x = Var(x)$\n",
    "\n",
    " - It is impossible to specify the minimizer unless the joint distribution of $x$ and $y$ is known\n",
    " \n",
    "  we do not know $\\mu_x\\,,\\mu_y\\,,\\sigma_x$ and $\\sigma_{xy}$\n",
    "  \n",
    "---  \n",
    "\n",
    "\n",
    "## (Homework) \n",
    "\n",
    "### Prove(6)\n",
    "\n",
    "- 회귀분석의 1차적인 목적은 표본으로부터 모회귀계수  $\\hat{B}^{pop}_0,\\hat{B}^{pop}_1$ 를 추정하여 추정된 회귀식을\n",
    "만드는 것이다.\n",
    "\n",
    "---\n",
    "\n",
    "- 최소 제곱법은 $\\sum_{i=1}^n(e_i)^2 = \\sum_{i=1}^n(y_i-(B_0 + B_1X_i))^2$ 식을 각각 $B_1$과 $B_0$로 각각 편미분하여 0과 같다고 놓는다. 그러면\n",
    "\n",
    "$$\\sum y_i = n\\hat B_0 + \\hat B_1\\sum x_i$$\n",
    "$$\\sum x_iy_i = \\hat B_0\\sum x_i + \\hat B_1\\sum x_i^2$$\n",
    "\n",
    "- 의 식이 나타난다. 이를 정리하면 \n",
    "\n",
    "$$\\hat B_0 = E(y) -\\hat B_1E(x)$$\n",
    "\n",
    "\n",
    "$$\\hat B_1 =\\frac{\\sum_{i=1}^n(x_i - E(x))(y_i - (E(y))}{ \\sum_{i=1}^n(x_i -E(x))^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
