{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  201111774 응용통계학과 박성진\n",
    "\n",
    "## (Homework)                                  \n",
    "\n",
    "# 1. Write an R code as above(LASSO)\n",
    "\n",
    "<img src=\"lasso.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Find and report some prorperties and algorithms for each loss(square, absolute, Huber, LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various loss functions\n",
    "\n",
    "- Popular choices for $L(B_0,B_1)$ for the simple linear regression model\n",
    "\n",
    "---\n",
    "\n",
    "## square loss (least squares estimation = L2 loss)\n",
    "\n",
    "$$L(B_0,B_1) = (y-B_0,B_1x)^2$$\n",
    "\n",
    "![loss](https://cdn-images-1.medium.com/max/800/1*EqTaoCB1NmJnsRYEezSACA.png)\n",
    "\n",
    "$$\\mbox{Plot of MSE Loss (Y-axis) vs. Predictions (X-axis)}$$\n",
    "\n",
    "---\n",
    "\n",
    "- Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "- It is always non-negative, and values closer to zero are better.\n",
    "\n",
    "- The mathematical benefits of mean squared error are particularly evident in its use at analyzing the performance of linear regression, as it allows one to partition the variation in a dataset into variation explained by the model and variation explained by randomness.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## absolute loss (Least absolute deviation  = L1 loss)\n",
    "\n",
    "$$L(B_0,B_1) = |\\,y − B_0 − B_1x\\,|$$ \n",
    "\n",
    "![L1_loss](https://cdn-images-1.medium.com/max/800/1*8BQhdKu1nk-tAAbOR17qGg.png)\n",
    "\n",
    "$$\\mbox{Plot of MAE Loss (Y-axis) vs. Predictions (X-axis)} $$\n",
    "\n",
    "---\n",
    "\n",
    "- The method of least absolute deviations finds applications in many areas, due to its robustness compared to the least squares method. \n",
    "\n",
    "- Least absolute deviations is robust in that it is resistant to outliers in the data. \n",
    "\n",
    "- LAD gives equal emphasis to all observations, in contrast to ordinary least squares (OLS) which, by squaring the residuals, gives more weight to large residuals, that is, outliers in which predicted values are far from actual observations.\n",
    "\n",
    "- This may be helpful in studies where outliers do not need to be given greater weight than other observations. \n",
    "\n",
    "- If it is important to give greater weight to outliers, the method of least squares is a better choice.\n",
    "\n",
    "\n",
    "## L1 vs L2\n",
    "\n",
    "![L1_vs_L2](https://cdn-images-1.medium.com/max/800/1*KibGRET1M6Bu0-8XmjviMA.png)\n",
    "$$\\mbox{Left: Errors are close to each other Right: One error is way off as compared to others\n",
    "}$$\n",
    "\n",
    "---\n",
    "\n",
    "- L1 loss function is more robust and is generally not affected by outliers. On the contrary L2 loss function will try to adjust the model according to these outlier values, even on the expense of other samples. Hence, L2 loss function is highly sensitive to outliers in the dataset.\n",
    "\n",
    "- With outliers in the dataset, a L2(Loss function) tries to adjust the model according to these outliers on the expense of other good-samples, since the squared-error is going to be huge for these outliers(for error > 1). On the other hand L1(Least absolute deviation) is quite resistant to outliers.\n",
    "\n",
    " As a result, L2 loss function may result in huge deviations in some of the samples which results in reduced accuracy.\n",
    "\n",
    "- So, if you can ignore the ouliers in your dataset or you need them to be there, then you should be using a L1 loss function, on the other hand if you don’t want undesired outliers in the dataset and would like to use a stable solution then first of all you should try to remove the outliers and then use a L2 loss function. Or performance of a model with a L2 loss function may deteriorate badly due to the presence of outliers in the dataset.\n",
    "\n",
    "#### summary \n",
    "\n",
    "#### L1 loss is more robust to outliers, but its derivatives are not continuous, making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0.)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Huber loss (Huberiezed estimation)\n",
    "$$L(B_0,B_1) = \\begin{cases}\n",
    "t^2, & \\mbox{if }|t|\\leq\\delta \\\\\n",
    "2\\delta|t|-\\delta^2, & \\mbox{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$∗ \\,t = y − B_0 − B_1x$\n",
    "\n",
    "$∗ \\,δ > 0: \\mbox{Huber’s tuning parameter (Huber’s recommendation δ = 1.345)}$\n",
    "\n",
    "\n",
    "![huber](https://cdn-images-1.medium.com/max/800/1*jxidxadWSMLvwLDZz2mycg.png)\n",
    "---\n",
    "\n",
    "- **Problems with both(L1, L2)**: There can be cases where neither loss function gives desirable predictions. For example, if 90% of observations in our data have true target value of 150 and the remaining 10% have target value between 0–30. \n",
    "\n",
    " Then a model with MAE as loss might predict 150 for all observations, ignoring 10% of outlier cases, as it will try to go towards median value. \n",
    " \n",
    "  In the same case, a model using MSE would give many predictions in the range of 0 to 30 as it will get skewed towards outliers. Both results are undesirable in many business cases.\n",
    "  \n",
    "\n",
    "- As defined above, the Huber loss function is strongly convex in a uniform neighborhood of its minimum  t=0; at the boundary of this uniform neighborhood, the Huber loss function has a differentiable extension to an affine function at points t= $\\delta$ and t= $-\\delta$\n",
    "\n",
    "\n",
    "- These properties allow it to combine much of the sensitivity of the mean-unbiased, minimum-variance estimator of the mean and the robustness of the median-unbiased estimator \n",
    "\n",
    "\n",
    "####  Summary:  \n",
    "\n",
    "#### Huber loss is less sensitive to outliers in data than the squared error loss. It’s also differentiable at 0. It’s basically absolute error, which becomes quadratic when error is small. \n",
    "\n",
    "#### How small that error has to be to make it quadratic depends on a hyperparameter, 𝛿 (delta), which can be tuned. Huber loss approaches MAE when 𝛿 ~ 0 and MSE when 𝛿 ~ ∞ (large numbers.)\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "#### references\n",
    "- https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "\n",
    "- https://nbviewer.jupyter.org/github/groverpr/Machine-Learning/blob/master/notebooks/05_Loss_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Normal equation(LSE) proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i번째 sample ($x_i$,$y_i$)에 관한 회귀모형을 다음으로 표현할 때\n",
    "\n",
    "- $$ y_i =  B_0 + B_1x_i +\\epsilon $$\n",
    "\n",
    "Loss 함수는 $L(B_0,B_1) = (y-B_0-B_1x)^2$ 과 같아서\n",
    "\n",
    "Risk 함수는 $R_n(B_0,B_1) =  \\sum_{i=1}^n(y_i-(B_0 + B_1x_i))^2/2n$ 이다.\n",
    "\n",
    "이를 최소로 하는 $B_0$과 $B_1$의 값을 이들의 추정값 $\\hat B_0, \\hat B_1$ 로 하는 방법이 **LSE** 이다.\n",
    "\n",
    "$R_n(B_0,B_1)$을 최소화 시키는 $B_0, B_1$을 구하기 위하여 $R_n(B_0,B_1)$을 $B_1$과 $B_0$로 각각 편미분하여 다음의 결과를 얻는다\n",
    "\n",
    "\n",
    "- $$\\nabla R_n(B_0, B_1) = \\begin{pmatrix} -\\sum_{i=1}^n(y_i-(B_0 + B_1x_i)/n  \\\\\n",
    "-\\sum_{i=1}^nx_i(y_i-(B_0 + B_1x_i)/n \\end{pmatrix}$$\n",
    "\n",
    "위의 편미분 값을 0으로 만드는 $B_0, B_1$을 $\\hat B_0, \\hat B_1$으로 대치하고 정리하면\n",
    "\n",
    "- $$ \\hat B_0n + \\hat B_1\\sum x_i = \\sum y_i$$\n",
    "\n",
    "- $$ \\hat B_0\\sum x_i + \\hat B_1\\sum x_i^2 = \\sum x_iy_i$$\n",
    "\n",
    "이 되는데 이 식을 **Normal equation**이라 부르고, 이를 $\\hat B_0$과 $\\hat B_1$에 대해 풀면,\n",
    "\n",
    "\n",
    "- $\\hat B_0 = \\bar y -\\hat B_1\\bar x$\n",
    "\n",
    "\n",
    "- $\\hat B_1 =\\frac{\\sum_{i=1}^n(x_i - \\bar x)(y_i - \\bar y)}{ \\sum_{i=1}^n(x_i -\\bar x)^2}$  이고,\n",
    "\n",
    "\n",
    "표현을 간단히 하기 위하여 \n",
    "\n",
    "\n",
    "- $S_{xx} = \\sum (x_i - \\bar x)^2$\n",
    "\n",
    "- $S_{yy} = \\sum (y_i - \\bar y)^2$\n",
    "\n",
    "- $S_{xy} = \\sum (x_i - \\bar x)(y_i - \\bar y)$ \n",
    "\n",
    "을 사용하기로 하면 \n",
    "\n",
    "\n",
    "\n",
    "- $\\hat B_0 = \\bar y - \\hat B_1\\bar x$\n",
    "\n",
    "- $\\hat B_1 = \\frac{S_{xy}}{S_{xx}}$ \n",
    "\n",
    "로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications for the Estimators\n",
    "\n",
    "- $\\hat B_1$ is normally distributed with mean $B_1$ and variance $\\sigma^2 \\over S_{xx}$\n",
    "\n",
    "- $\\hat B_0$ is normally distributed with mean $B_0$ and variance $\\sigma^2 \\frac{\\sum_{i=1}^nx_i^2}{nS_{xx}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ANOVA TABLE\n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th align=\"center\">Source</th>\n",
    "      <th align=\"center\">SS</th>\n",
    "      <th align=\"center\">DF</th>\n",
    "      <th align=\"center\">MS</th>\n",
    "      <th align=\"center\">F</th>\n",
    "      <th align=\"center\">P</th>  \n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">Factor</td>\n",
    "      <th align=\"center\">SSR</th>\n",
    "      <th align=\"center\">1</th>\n",
    "      <th align=\"center\">SSR</th>\n",
    "      <th align=\"center\">MSR/MSE</th>\n",
    "      <th align=\"center\">F(1, n-2; a)</th>    \n",
    "   </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">Error</td>\n",
    "      <th align=\"center\">SSE</th>\n",
    "      <th align=\"center\">n-2</th>\n",
    "      <th align=\"center\">MSE$=\\frac{SSE}{n-2}$</th>    \n",
    "\n",
    "   </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">Total</td>\n",
    "      <td align=\"center\">SST</td>\n",
    "      <td align=\"center\">n-1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. AIC(Akaike's Information Criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AIC는 여러 통계 모델들의 성능을 서로 비교할 수 있게 해줌.\n",
    "\n",
    "\n",
    "- 데이터에 대한 여러 모델 세트가 주어지면 더 나은 모델은 최소 AIC 값을 갖는 모델.\n",
    "\n",
    "- 따라서 AIC는 goodness of fit(가능도에 의해 구해진)을 보장하지만, 추정된 회귀계수의 수에 따라 증가하는 페널티도 포함.\n",
    "\n",
    "- 모델에서 설명변수의 수를 늘리면 거의 goodness of fit이 향상하므로, 패널티는 overfitting을 방지함.\n",
    "\n",
    "- AIC 값은 두 모델의 관측치 개수가 거의 동일할 때만 비교해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proof (11)\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "### 1. The distribution of X is arbitrary (and perhaps X is even non-random).\n",
    "\n",
    "### 2. If X = x, then Y = β0 + β1x + $\\epsilon$ , for some constants (“coefficients”, “parameters”) β0 and β1, and some random noise variable \n",
    "\n",
    "### 3.  $\\epsilon$∼ N(0, σ2), and is independent of X.\n",
    "\n",
    "### 4. $\\epsilon$ is independent across observations\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Because of these stronger assumptions, the model tells us the conditional pdf of Y for each x, p(y|X = x; β0, β1, σ2)\n",
    "\n",
    "- Given any data set (x1, y1),(x2, y2), . . .(xn, yn), we can now write down the probability density, under the model, of seeing that data:\n",
    "\n",
    "$$\t\\prod_{i=1}^n p(y_i\\,|\\,xi; β_0, β_1, σ^2)=\\prod_{i=1}^n \\frac{1}{\\sqrt {2\\pi\\sigma^2 }}e^\\frac{{ - \\left( {y_i - (B_0 +B_1x_i) } \\right)^2 }}{2\\sigma^2}$$\n",
    "\n",
    "#### references\n",
    "\n",
    "- https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d\n",
    "\n",
    "- https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
