{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  201111774 ì‘ìš©í†µê³„í•™ê³¼ ë°•ì„±ì§„\n",
    "\n",
    "## (Homework)                                  \n",
    "\n",
    "# 1. Write an R code as above(LASSO)\n",
    "\n",
    "<img src=\"lasso.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Find and report some prorperties and algorithms for each loss(square, absolute, Huber, LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various loss functions\n",
    "\n",
    "- Popular choices for $L(B_0,B_1)$ for the simple linear regression model\n",
    "\n",
    "---\n",
    "\n",
    "## square loss (least squares estimation = L2 loss)\n",
    "\n",
    "$$L(B_0,B_1) = (y-B_0,B_1x)^2$$\n",
    "\n",
    "![loss](https://cdn-images-1.medium.com/max/800/1*EqTaoCB1NmJnsRYEezSACA.png)\n",
    "\n",
    "$$\\mbox{Plot of MSE Loss (Y-axis) vs. Predictions (X-axis)}$$\n",
    "\n",
    "---\n",
    "\n",
    "- Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "- It is always non-negative, and values closer to zero are better.\n",
    "\n",
    "- The mathematical benefits of mean squared error are particularly evident in its use at analyzing the performance of linear regression, as it allows one to partition the variation in a dataset into variation explained by the model and variation explained by randomness.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## absolute loss (Least absolute deviation  = L1 loss)\n",
    "\n",
    "$$L(B_0,B_1) = |\\,y âˆ’ B_0 âˆ’ B_1x\\,|$$ \n",
    "\n",
    "![L1_loss](https://cdn-images-1.medium.com/max/800/1*8BQhdKu1nk-tAAbOR17qGg.png)\n",
    "\n",
    "$$\\mbox{Plot of MAE Loss (Y-axis) vs. Predictions (X-axis)} $$\n",
    "\n",
    "---\n",
    "\n",
    "- The method of least absolute deviations finds applications in many areas, due to its robustness compared to the least squares method. \n",
    "\n",
    "- Least absolute deviations is robust in that it is resistant to outliers in the data. \n",
    "\n",
    "- LAD gives equal emphasis to all observations, in contrast to ordinary least squares (OLS) which, by squaring the residuals, gives more weight to large residuals, that is, outliers in which predicted values are far from actual observations.\n",
    "\n",
    "- This may be helpful in studies where outliers do not need to be given greater weight than other observations. \n",
    "\n",
    "- If it is important to give greater weight to outliers, the method of least squares is a better choice.\n",
    "\n",
    "\n",
    "## L1 vs L2\n",
    "\n",
    "![L1_vs_L2](https://cdn-images-1.medium.com/max/800/1*KibGRET1M6Bu0-8XmjviMA.png)\n",
    "$$\\mbox{Left: Errors are close to each other Right: One error is way off as compared to others\n",
    "}$$\n",
    "\n",
    "---\n",
    "\n",
    "- L1 loss function is more robust and is generally not affected by outliers. On the contrary L2 loss function will try to adjust the model according to these outlier values, even on the expense of other samples. Hence, L2 loss function is highly sensitive to outliers in the dataset.\n",
    "\n",
    "- With outliers in the dataset, a L2(Loss function) tries to adjust the model according to these outliers on the expense of other good-samples, since the squared-error is going to be huge for these outliers(for error > 1). On the other hand L1(Least absolute deviation) is quite resistant to outliers.\n",
    "\n",
    " As a result, L2 loss function may result in huge deviations in some of the samples which results in reduced accuracy.\n",
    "\n",
    "- So, if you can ignore the ouliers in your dataset or you need them to be there, then you should be using a L1 loss function, on the other hand if you donâ€™t want undesired outliers in the dataset and would like to use a stable solution then first of all you should try to remove the outliers and then use a L2 loss function. Or performance of a model with a L2 loss function may deteriorate badly due to the presence of outliers in the dataset.\n",
    "\n",
    "#### summary \n",
    "\n",
    "#### L1 loss is more robust to outliers, but its derivatives are not continuous, making it inefficient to find the solution. L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0.)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Huber loss (Huberiezed estimation)\n",
    "$$L(B_0,B_1) = \\begin{cases}\n",
    "t^2, & \\mbox{if }|t|\\leq\\delta \\\\\n",
    "2\\delta|t|-\\delta^2, & \\mbox{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$âˆ— \\,t = y âˆ’ B_0 âˆ’ B_1x$\n",
    "\n",
    "$âˆ— \\,Î´ > 0: \\mbox{Huberâ€™s tuning parameter (Huberâ€™s recommendation Î´ = 1.345)}$\n",
    "\n",
    "\n",
    "![huber](https://cdn-images-1.medium.com/max/800/1*jxidxadWSMLvwLDZz2mycg.png)\n",
    "---\n",
    "\n",
    "- **Problems with both(L1, L2)**: There can be cases where neither loss function gives desirable predictions. For example, if 90% of observations in our data have true target value of 150 and the remaining 10% have target value between 0â€“30. \n",
    "\n",
    " Then a model with MAE as loss might predict 150 for all observations, ignoring 10% of outlier cases, as it will try to go towards median value. \n",
    " \n",
    "  In the same case, a model using MSE would give many predictions in the range of 0 to 30 as it will get skewed towards outliers. Both results are undesirable in many business cases.\n",
    "  \n",
    "\n",
    "- As defined above, the Huber loss function is strongly convex in a uniform neighborhood of its minimum  t=0; at the boundary of this uniform neighborhood, the Huber loss function has a differentiable extension to an affine function at points t= $\\delta$ and t= $-\\delta$\n",
    "\n",
    "\n",
    "- These properties allow it to combine much of the sensitivity of the mean-unbiased, minimum-variance estimator of the mean and the robustness of the median-unbiased estimator \n",
    "\n",
    "\n",
    "####  Summary:  \n",
    "\n",
    "#### Huber loss is less sensitive to outliers in data than the squared error loss. Itâ€™s also differentiable at 0. Itâ€™s basically absolute error, which becomes quadratic when error is small. \n",
    "\n",
    "#### How small that error has to be to make it quadratic depends on a hyperparameter, ğ›¿ (delta), which can be tuned. Huber loss approaches MAE when ğ›¿ ~ 0 and MSE when ğ›¿ ~ âˆ (large numbers.)\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "#### references\n",
    "- https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "\n",
    "- https://nbviewer.jupyter.org/github/groverpr/Machine-Learning/blob/master/notebooks/05_Loss_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Normal equation(LSE) proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ië²ˆì§¸ sample ($x_i$,$y_i$)ì— ê´€í•œ íšŒê·€ëª¨í˜•ì„ ë‹¤ìŒìœ¼ë¡œ í‘œí˜„í•  ë•Œ\n",
    "\n",
    "- $$ y_i =  B_0 + B_1x_i +\\epsilon $$\n",
    "\n",
    "Loss í•¨ìˆ˜ëŠ” $L(B_0,B_1) = (y-B_0-B_1x)^2$ ê³¼ ê°™ì•„ì„œ\n",
    "\n",
    "Risk í•¨ìˆ˜ëŠ” $R_n(B_0,B_1) =  \\sum_{i=1}^n(y_i-(B_0 + B_1x_i))^2/2n$ ì´ë‹¤.\n",
    "\n",
    "ì´ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” $B_0$ê³¼ $B_1$ì˜ ê°’ì„ ì´ë“¤ì˜ ì¶”ì •ê°’ $\\hat B_0, \\hat B_1$ ë¡œ í•˜ëŠ” ë°©ë²•ì´ **LSE** ì´ë‹¤.\n",
    "\n",
    "$R_n(B_0,B_1)$ì„ ìµœì†Œí™” ì‹œí‚¤ëŠ” $B_0, B_1$ì„ êµ¬í•˜ê¸° ìœ„í•˜ì—¬ $R_n(B_0,B_1)$ì„ $B_1$ê³¼ $B_0$ë¡œ ê°ê° í¸ë¯¸ë¶„í•˜ì—¬ ë‹¤ìŒì˜ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤\n",
    "\n",
    "\n",
    "- $$\\nabla R_n(B_0, B_1) = \\begin{pmatrix} -\\sum_{i=1}^n(y_i-(B_0 + B_1x_i)/n  \\\\\n",
    "-\\sum_{i=1}^nx_i(y_i-(B_0 + B_1x_i)/n \\end{pmatrix}$$\n",
    "\n",
    "ìœ„ì˜ í¸ë¯¸ë¶„ ê°’ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” $B_0, B_1$ì„ $\\hat B_0, \\hat B_1$ìœ¼ë¡œ ëŒ€ì¹˜í•˜ê³  ì •ë¦¬í•˜ë©´\n",
    "\n",
    "- $$ \\hat B_0n + \\hat B_1\\sum x_i = \\sum y_i$$\n",
    "\n",
    "- $$ \\hat B_0\\sum x_i + \\hat B_1\\sum x_i^2 = \\sum x_iy_i$$\n",
    "\n",
    "ì´ ë˜ëŠ”ë° ì´ ì‹ì„ **Normal equation**ì´ë¼ ë¶€ë¥´ê³ , ì´ë¥¼ $\\hat B_0$ê³¼ $\\hat B_1$ì— ëŒ€í•´ í’€ë©´,\n",
    "\n",
    "\n",
    "- $\\hat B_0 = \\bar y -\\hat B_1\\bar x$\n",
    "\n",
    "\n",
    "- $\\hat B_1 =\\frac{\\sum_{i=1}^n(x_i - \\bar x)(y_i - \\bar y)}{ \\sum_{i=1}^n(x_i -\\bar x)^2}$  ì´ê³ ,\n",
    "\n",
    "\n",
    "í‘œí˜„ì„ ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•˜ì—¬ \n",
    "\n",
    "\n",
    "- $S_{xx} = \\sum (x_i - \\bar x)^2$\n",
    "\n",
    "- $S_{yy} = \\sum (y_i - \\bar y)^2$\n",
    "\n",
    "- $S_{xy} = \\sum (x_i - \\bar x)(y_i - \\bar y)$ \n",
    "\n",
    "ì„ ì‚¬ìš©í•˜ê¸°ë¡œ í•˜ë©´ \n",
    "\n",
    "\n",
    "\n",
    "- $\\hat B_0 = \\bar y - \\hat B_1\\bar x$\n",
    "\n",
    "- $\\hat B_1 = \\frac{S_{xy}}{S_{xx}}$ \n",
    "\n",
    "ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications for the Estimators\n",
    "\n",
    "- $\\hat B_1$ is normally distributed with mean $B_1$ and variance $\\sigma^2 \\over S_{xx}$\n",
    "\n",
    "- $\\hat B_0$ is normally distributed with mean $B_0$ and variance $\\sigma^2 \\frac{\\sum_{i=1}^nx_i^2}{nS_{xx}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ANOVA TABLE\n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th align=\"center\">Source</th>\n",
    "      <th align=\"center\">SS</th>\n",
    "      <th align=\"center\">DF</th>\n",
    "      <th align=\"center\">MS</th>\n",
    "      <th align=\"center\">F</th>\n",
    "      <th align=\"center\">P</th>  \n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">Factor</td>\n",
    "      <th align=\"center\">SSR</th>\n",
    "      <th align=\"center\">1</th>\n",
    "      <th align=\"center\">SSR</th>\n",
    "      <th align=\"center\">MSR/MSE</th>\n",
    "      <th align=\"center\">F(1, n-2; a)</th>    \n",
    "   </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">Error</td>\n",
    "      <th align=\"center\">SSE</th>\n",
    "      <th align=\"center\">n-2</th>\n",
    "      <th align=\"center\">MSE$=\\frac{SSE}{n-2}$</th>    \n",
    "\n",
    "   </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">Total</td>\n",
    "      <td align=\"center\">SST</td>\n",
    "      <td align=\"center\">n-1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. AIC(Akaike's Information Criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AICëŠ” ì—¬ëŸ¬ í†µê³„ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ì„œë¡œ ë¹„êµí•  ìˆ˜ ìˆê²Œ í•´ì¤Œ.\n",
    "\n",
    "\n",
    "- ë°ì´í„°ì— ëŒ€í•œ ì—¬ëŸ¬ ëª¨ë¸ ì„¸íŠ¸ê°€ ì£¼ì–´ì§€ë©´ ë” ë‚˜ì€ ëª¨ë¸ì€ ìµœì†Œ AIC ê°’ì„ ê°–ëŠ” ëª¨ë¸.\n",
    "\n",
    "- ë”°ë¼ì„œ AICëŠ” goodness of fit(ê°€ëŠ¥ë„ì— ì˜í•´ êµ¬í•´ì§„)ì„ ë³´ì¥í•˜ì§€ë§Œ, ì¶”ì •ëœ íšŒê·€ê³„ìˆ˜ì˜ ìˆ˜ì— ë”°ë¼ ì¦ê°€í•˜ëŠ” í˜ë„í‹°ë„ í¬í•¨.\n",
    "\n",
    "- ëª¨ë¸ì—ì„œ ì„¤ëª…ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ê±°ì˜ goodness of fitì´ í–¥ìƒí•˜ë¯€ë¡œ, íŒ¨ë„í‹°ëŠ” overfittingì„ ë°©ì§€í•¨.\n",
    "\n",
    "- AIC ê°’ì€ ë‘ ëª¨ë¸ì˜ ê´€ì¸¡ì¹˜ ê°œìˆ˜ê°€ ê±°ì˜ ë™ì¼í•  ë•Œë§Œ ë¹„êµí•´ì•¼í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proof (11)\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "### 1. The distribution of X is arbitrary (and perhaps X is even non-random).\n",
    "\n",
    "### 2. If X = x, then Y = Î²0 + Î²1x + $\\epsilon$ , for some constants (â€œcoefficientsâ€, â€œparametersâ€) Î²0 and Î²1, and some random noise variable \n",
    "\n",
    "### 3.  $\\epsilon$âˆ¼ N(0, Ïƒ2), and is independent of X.\n",
    "\n",
    "### 4. $\\epsilon$ is independent across observations\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Because of these stronger assumptions, the model tells us the conditional pdf of Y for each x, p(y|X = x; Î²0, Î²1, Ïƒ2)\n",
    "\n",
    "- Given any data set (x1, y1),(x2, y2), . . .(xn, yn), we can now write down the probability density, under the model, of seeing that data:\n",
    "\n",
    "$$\t\\prod_{i=1}^n p(y_i\\,|\\,xi; Î²_0, Î²_1, Ïƒ^2)=\\prod_{i=1}^n \\frac{1}{\\sqrt {2\\pi\\sigma^2 }}e^\\frac{{ - \\left( {y_i - (B_0 +B_1x_i) } \\right)^2 }}{2\\sigma^2}$$\n",
    "\n",
    "#### references\n",
    "\n",
    "- https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d\n",
    "\n",
    "- https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
